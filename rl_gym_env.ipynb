{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_gym_env.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW6u6ugGoB-o"
      },
      "source": [
        "# Reinforcement learning applied to Gym environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lZsgQuT0sns"
      },
      "source": [
        "This notebook should be run on a local runtime (most tests take a very long time to complete).\n",
        "\n",
        "List of Python dependencies:\n",
        "```\n",
        "pip install 'imageio==2.4.0'\n",
        "pip install pyvirtualdisplay\n",
        "pip install numpy\n",
        "pip install tensorflow\n",
        "pip install tf_agents\n",
        "pip install pillow\n",
        "pip install matplotlib\n",
        "pip install cv2\n",
        "pip install conda\n",
        "pip install opencv-python\n",
        "pip install gym[box2d]\n",
        "```\n",
        "\n",
        "To start a local jupyter notebook:\n",
        "```\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD75jk3tTNWH"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import gym\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model as tf_load_model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import backend as k_backend\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, linear\n",
        "from tensorflow.keras.losses import mean_squared_error, Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.models import clone_model\n",
        "from tensorflow import where as tf_where\n",
        "from collections import deque\n",
        "from scipy.stats import multivariate_normal\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import base64\n",
        "from PIL import Image, ImageDraw\n",
        "import IPython\n",
        "\n",
        "FIGSIZE = (20, 10)\n",
        "FONTSIZE = 14\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHyqznPYxLyO"
      },
      "source": [
        "# \"Classic\" DQN, DQN with Target newtork, Double DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0_a1BuKS5DY"
      },
      "source": [
        "def masked_huber_loss(mask_value, clip_delta):\n",
        "  def f(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    cond = k_backend.abs(error) < clip_delta\n",
        "    mask_true = k_backend.cast(k_backend.not_equal(y_true, mask_value), k_backend.floatx())\n",
        "    masked_squared_error = 0.5 * k_backend.square(mask_true * (y_true - y_pred))\n",
        "    linear_loss  = mask_true * (clip_delta * k_backend.abs(error) - 0.5 * (clip_delta ** 2))\n",
        "    huber_loss = tf_where(cond, masked_squared_error, linear_loss)\n",
        "    return k_backend.sum(huber_loss) / k_backend.sum(mask_true)\n",
        "  f.__name__ = 'masked_huber_loss'\n",
        "  return f\n",
        "\n",
        "def masked_mean_squared_error(mask_value, clip_delta):\n",
        "  def f(y_true, y_pred):\n",
        "    mask_true = k_backend.cast(k_backend.not_equal(y_true, mask_value), k_backend.floatx())\n",
        "    if clip_delta > 0:\n",
        "      masked_squared_error = k_backend.clip(k_backend.square(mask_true * (y_true - y_pred)), (-1) * clip_delta, clip_delta)\n",
        "    else:\n",
        "      masked_squared_error = k_backend.square(mask_true * (y_true - y_pred))\n",
        "    return k_backend.sum(masked_squared_error) / k_backend.sum(mask_true)\n",
        "  f.__name__ = 'masked_mean_squared_error'\n",
        "  return f\n",
        "\n",
        "# This object must be passed as parameter `custom_objects` when loading a saved model\n",
        "CUSTOM_OBJECTS = {'masked_huber_loss': masked_huber_loss, 'masked_mean_squared_error': masked_mean_squared_error}\n",
        "\n",
        "def get_output_input_dim_from_env(env):\n",
        "  output_dim = env.action_space.n\n",
        "  input_dim = env.observation_space.shape[0]\n",
        "  return output_dim, input_dim\n",
        "\n",
        "def create_dqn_network(hyperparams, env):\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "  model = Sequential()\n",
        "  # Input layer -> first layer\n",
        "  model.add(Dense(hyperparams['layer_params'][0], input_dim=input_dim, activation=relu))\n",
        "  # Rest of hidden layers\n",
        "  for lr in hyperparams['layer_params'][1:]:\n",
        "    model.add(Dense(lr, activation=relu))\n",
        "  # Output layer\n",
        "  model.add(Dense(output_dim, activation=linear))\n",
        "\n",
        "  # Compile model\n",
        "  optimizer = None\n",
        "  if hyperparams['optimizer'] == 'Adam':\n",
        "    optimizer = Adam(learning_rate=hyperparams['learning_rate'])\n",
        "\n",
        "  loss = None\n",
        "  if hyperparams['loss'] == 'MSE':\n",
        "    if hyperparams['error_clipping']:\n",
        "      loss = masked_mean_squared_error(0.0, 1.0)\n",
        "    else:\n",
        "      loss = masked_mean_squared_error(0.0, 0)\n",
        "  elif hyperparams['loss'] == 'Huber':\n",
        "    loss = masked_huber_loss(0.0, 1.0)\n",
        "  model.compile(loss=loss, optimizer=optimizer)\n",
        "  return model\n",
        "\n",
        "def create_replay_buffer(hyperparams):\n",
        "  buffer = deque(maxlen=hyperparams['max_buffer_size'])\n",
        "  return buffer\n",
        "\n",
        "# Sample should be a tuple: (state, action, reward, next_state, done)\n",
        "def add_sample_to_buffer(hyperparams, buffer, sample):\n",
        "  # Deque automatically removes elements from opposite end when maxlen is reached\n",
        "  buffer.append(sample)\n",
        "\n",
        "def get_batch_from_buffer(buffer, batch_size):\n",
        "  sample_batch = random.sample(buffer, batch_size)\n",
        "  return sample_batch\n",
        "\n",
        "def get_action(hyperparams, episode, state, epsilon, model, max_action):\n",
        "  # Explore: select random action with probability epsilon\n",
        "  if episode < hyperparams['explore_only_episodes'] or np.random.rand() < epsilon:\n",
        "    return random.randrange(max_action)\n",
        "  # Exploit: select best predicted action\n",
        "  exploit_actions = model.predict(state)\n",
        "  return np.argmax(exploit_actions[0])\n",
        "\n",
        "def extract_data_from_batch(batch):\n",
        "  states = np.array([i[0] for i in batch])\n",
        "  actions = np.array([i[1] for i in batch])\n",
        "  rewards = np.array([i[2] for i in batch])\n",
        "  next_states = np.array([i[3] for i in batch])\n",
        "  dones = np.array([i[4] for i in batch])\n",
        "  # Squeeze: remove axes of length 1 from array\n",
        "  states = np.squeeze(states)\n",
        "  next_states = np.squeeze(next_states)\n",
        "  return states, actions, rewards, next_states, dones\n",
        "\n",
        "def learn(hyperparams, output_dim, rewards, buffer, model, target_model):\n",
        "  sample_batch = get_batch_from_buffer(buffer, hyperparams['batch_size'])\n",
        "  states, actions, rewards, next_states, dones = extract_data_from_batch(sample_batch)\n",
        "  target_vec = None\n",
        "  \n",
        "  # Double Q-learning\n",
        "  if hyperparams['double_q_learning']:\n",
        "    target_vec = np.zeros((hyperparams['batch_size'], output_dim))\n",
        "\n",
        "    # Calc Q(s', a')\n",
        "    model_ns_predictions = model.predict_on_batch(next_states)\n",
        "\n",
        "    # Calc Q'(s', a')\n",
        "    target_model_ns_predictions = target_model.predict_on_batch(next_states)\n",
        "\n",
        "    indexes = np.array([i for i in range(hyperparams['batch_size'])])\n",
        "\n",
        "    targets = rewards + hyperparams['gamma'] * target_model_ns_predictions[[indexes], np.argmax(model_ns_predictions, axis=1)] * (1 - dones)\n",
        "    \n",
        "    target_vec[[indexes], [actions]] = targets\n",
        "  else:\n",
        "    # Calc array of yj\n",
        "    # Note: set yj = rj for terminal states (1 - done = 0)\n",
        "    targets = None\n",
        "    if hyperparams['use_target_network']:\n",
        "    # otherwise set yj = rj + gamma * (max Q value on next state using target model)\n",
        "      targets = rewards + hyperparams['gamma'] * (np.amax(target_model.predict_on_batch(next_states), axis=1)) * (1 - dones)\n",
        "    else:\n",
        "      targets = rewards + hyperparams['gamma'] * (np.amax(model.predict_on_batch(next_states), axis=1)) * (1 - dones)\n",
        "    \n",
        "    # Calc Q value for current states\n",
        "    # This is probably better because we are using a different network to produce the targets \n",
        "    if hyperparams['use_target_network']:\n",
        "      target_vec = np.zeros((hyperparams['batch_size'], output_dim))\n",
        "    else:\n",
        "      target_vec = model.predict_on_batch(states)\n",
        "\n",
        "    indexes = np.array([i for i in range(hyperparams['batch_size'])])\n",
        "    # Substitute yj as expected value for each action\n",
        "    target_vec[[indexes], [actions]] = targets\n",
        "\n",
        "  model.fit(states, target_vec, epochs=1, batch_size=hyperparams['batch_size'], verbose=0)\n",
        "\n",
        "def train_done(hyperparams, episode, iteration):\n",
        "  if 'max_train_iterations' in hyperparams:\n",
        "    if iteration >= hyperparams['max_train_iterations']:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    return episode >= hyperparams['train_episodes']\n",
        "\n",
        "def explore_only(hyperparams, episode, buffer):\n",
        "  # Set an explicit number of explore only steps\n",
        "  if hyperparams['explore_only_episodes'] >= 0:\n",
        "    if episode < hyperparams['explore_only_episodes']:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  # Or at least wait until buffer is full enough to train\n",
        "  elif len(buffer) < hyperparams['batch_size'] * 2:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def train(hyperparams, env, eval_env, model, target_model, buffer):\n",
        "  start_time = current_ms()\n",
        "  # Global iteration counter\n",
        "  iteration = 0\n",
        "  episode = -1\n",
        "  # List of episode rewards\n",
        "  episode_rewards = []\n",
        "  mean_rewards = []\n",
        "  visited_states = []\n",
        "  eval_returns = []\n",
        "  epsilon = hyperparams['epsilon_start']\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "\n",
        "  while not train_done(hyperparams, episode, iteration):\n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "    visited_states.append(state)\n",
        "    state = np.reshape(state, [1, input_dim])\n",
        "    for step in range(hyperparams['max_episode_steps']):\n",
        "      action = get_action(hyperparams, episode, state, epsilon, model, output_dim)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      if hyperparams['reward_clipping']:\n",
        "        if reward < -1:\n",
        "          reward = -1\n",
        "        elif reward > 1:\n",
        "          reward = 1\n",
        "      visited_states.append(next_state)\n",
        "      episode_reward += reward\n",
        "      next_state = np.reshape(next_state, [1, input_dim])\n",
        "      sample = (state, action, reward, next_state, done)\n",
        "      add_sample_to_buffer(hyperparams, buffer, sample)\n",
        "      state = next_state\n",
        "      iteration += 1\n",
        "\n",
        "      if not explore_only(hyperparams, episode, buffer) and iteration % hyperparams['update_frequency'] == 0:\n",
        "        learn(hyperparams, output_dim, episode_rewards, buffer, model, target_model)\n",
        "        \n",
        "      # Every C steps set Q' = Q\n",
        "      if hyperparams['use_target_network'] and not explore_only(hyperparams, episode, buffer) and iteration % hyperparams['target_update_steps']:\n",
        "        target_model.set_weights(model.get_weights())\n",
        "      \n",
        "      if hyperparams['eval_interval'] > 0 and iteration % hyperparams['eval_interval'] == 0:\n",
        "        avg_returns, _ = eval_model(hyperparams, model, eval_env)\n",
        "        eval_returns.append(avg_returns)\n",
        "        print(\">>> Iteration {0}, Episode {1}, Avg. returns {2}\".format(iteration, episode, avg_returns))\n",
        "\n",
        "      # Episode done\n",
        "      if done:\n",
        "        break\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    # Save model to fs\n",
        "    if hyperparams['save_model_every_n_episodes'] > 0 and episode % hyperparams['save_model_every_n_episodes'] == 0:\n",
        "      model_name = hyperparams['model_name'] + '_ep' + str(episode)\n",
        "      if episode_reward > hyperparams['episode_solved_score']:\n",
        "        model_name += '_SOLVED'\n",
        "      save_model_to_fs(model, hyperparams['env_name'], model_name)\n",
        "    elif hyperparams['always_save_optimal_models'] and episode_reward > hyperparams['episode_solved_score']:\n",
        "      model_name = hyperparams['model_name'] + '_ep' + str(episode) + '_SOLVED'\n",
        "      save_model_to_fs(model, hyperparams['env_name'], model_name)\n",
        "\n",
        "    # Epsilon decay\n",
        "    if not explore_only(hyperparams, episode, buffer) and hyperparams['epsilon_decay'] > 0 and epsilon > hyperparams['epsilon_end']:\n",
        "      epsilon *= hyperparams['epsilon_decay']\n",
        "    \n",
        "    # Early stopping\n",
        "    last_rewards_mean = np.mean(episode_rewards[hyperparams['rewards_mean_episode_limit']:])\n",
        "    mean_rewards.append(last_rewards_mean)\n",
        "    if hyperparams['early_stopping'] and last_rewards_mean > hyperparams['episode_solved_score']:\n",
        "        print(\"Training complete with early stopping\")\n",
        "        break\n",
        "\n",
        "    if hyperparams['log_episodes']:\n",
        "      print(\"Iteration {0}, Episode {1}, Reward {2}, Avg. reward {3}, epsilon {4}\".format(iteration, episode, episode_reward, last_rewards_mean, epsilon))\n",
        "  end_time = current_ms()\n",
        "  print(\"Evaluation done in\", print_minutes(start_time, end_time))\n",
        "  return episode_rewards, mean_rewards, eval_returns, visited_states\n",
        "\n",
        "def eval_random_agent(hyperparams, env, n_tests=100):\n",
        "  start_time = current_ms()\n",
        "  rewards = []\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "  for _ in range(n_tests):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, input_dim])\n",
        "    episode_reward = 0\n",
        "    for _ in range(hyperparams['max_episode_steps']):\n",
        "      action = random.randrange(output_dim)\n",
        "      new_state, reward, done, _ = env.step(action)\n",
        "      new_state = np.reshape(new_state, [1, input_dim])\n",
        "      state = new_state\n",
        "      episode_reward += reward\n",
        "      if done:\n",
        "          break\n",
        "    rewards.append(episode_reward)\n",
        "  end_time = current_ms()\n",
        "  print(\"Evaluation done in\", print_minutes(start_time, end_time))\n",
        "  return np.mean(rewards), rewards\n",
        "\n",
        "def eval_model(hyperparams, model, env, n_tests=100):\n",
        "  start_time = current_ms()\n",
        "  rewards = []\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "  for _ in range(n_tests):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, input_dim])\n",
        "    episode_reward = 0\n",
        "    for _ in range(hyperparams['max_episode_steps']):\n",
        "      action = np.argmax(model.predict(state)[0])\n",
        "      new_state, reward, done, _ = env.step(action)\n",
        "      new_state = np.reshape(new_state, [1, input_dim])\n",
        "      state = new_state\n",
        "      episode_reward += reward\n",
        "      if done:\n",
        "          break\n",
        "    rewards.append(episode_reward)\n",
        "  end_time = current_ms()\n",
        "  print(\"Evaluation done in\", print_minutes(start_time, end_time))\n",
        "  return np.mean(rewards), rewards\n",
        "\n",
        "def create_model_eval_graph(model, env, epsilon, states_to_plot, n_steps):\n",
        "  plt.figure(figsize=FIGSIZE)\n",
        "  rewards = []\n",
        "  states = []\n",
        "  actions = []\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "  state = env.reset()\n",
        "  states.append(state)\n",
        "  state = np.reshape(state, [1, input_dim])\n",
        "  for _ in range(n_steps):\n",
        "    action = None\n",
        "    if np.random.rand() < epsilon:\n",
        "      action = random.randrange(output_dim)\n",
        "    else:\n",
        "      action = np.argmax(model.predict(state)[0])\n",
        "    actions.append(action)\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    states.append(new_state)\n",
        "    rewards.append(reward)\n",
        "    new_state = np.reshape(new_state, [1, input_dim])\n",
        "    state = new_state\n",
        "    if done:\n",
        "        break\n",
        "  for st in states_to_plot:\n",
        "    plt.plot([i for i in range(len(states))], [state[st['idx']] for state in states], label=st['label'])\n",
        "\n",
        "  plt.scatter([i for i in range(len(actions))], [action for action in actions], label='Azione')\n",
        "  plt.yticks(fontsize=FONTSIZE)\n",
        "  plt.xticks(fontsize=FONTSIZE)\n",
        "  plt.legend(prop={'size':FONTSIZE})\n",
        "  return \n",
        "\n",
        "def plot_model_evaluation(avg_reward, rewards):\n",
        "  plt.figure(figsize=FIGSIZE)\n",
        "  plt.plot([i for i in range(len(rewards))], rewards)\n",
        "  plt.plot([i for i in range(len(rewards))], [avg_reward for _ in range(len(rewards))])\n",
        "  plt.xlabel('Episodi', fontsize=FONTSIZE)\n",
        "  plt.ylabel('Ricompense', fontsize=FONTSIZE)\n",
        "  plt.yticks(fontsize=FONTSIZE)\n",
        "  plt.xticks(fontsize=FONTSIZE)\n",
        "\n",
        "def run_single_test(hyperparams, env):\n",
        "  model = create_dqn_network(hyperparams, env)\n",
        "  target_model = None\n",
        "  if hyperparams['use_target_network']:\n",
        "    target_model = create_dqn_network(hyperparams, env)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "  buffer = create_replay_buffer(hyperparams)\n",
        "  rewards, mean_rewards, eval_returns, visited_states = train(hyperparams, env, None, model, target_model, buffer)\n",
        "  return rewards, mean_rewards, eval_returns, visited_states\n",
        "\n",
        "# Run multiple tests with the same hyperparameters\n",
        "def run_multiple_tests(hyperparams, env, seed, n_tests=3):\n",
        "  # Reset seeds for reproducibility\n",
        "  env.seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  global_rewards = []\n",
        "  global_mean_rewards = []\n",
        "  global_states = []\n",
        "  for _ in range(n_tests):\n",
        "    rewards, mean_rewards, eval_returns, visited_states = run_single_test(hyperparams, env)\n",
        "\n",
        "    global_rewards.append(rewards)\n",
        "    global_mean_rewards.append(mean_rewards)\n",
        "    global_states.append(visited_states)\n",
        "  return global_rewards, global_mean_rewards, global_states\n",
        "\n",
        "def generate_hyperparams_for_tests(base_hyperparams, varying_vals):\n",
        "  result = []\n",
        "  for key in varying_vals:\n",
        "    for i in range(len(varying_vals[key])):\n",
        "      new_hyper = base_hyperparams.copy()\n",
        "      new_hyper['model_name'] = base_hyperparams['model_name'] + '_' + key + '_' + i\n",
        "      new_hyper[key] = varying_vals[key][i]\n",
        "      result.append(new_hyper)\n",
        "  return result\n",
        "\n",
        "# Test different hyperparameters on the same environment (seed reset for each new test)\n",
        "# Note: if models should be saved change model_name for each hyperparameters dictionary in the list\n",
        "def run_different_hyperparams(hyper_list, env, seed, test_name):\n",
        "  test = 0\n",
        "  for hyperparams in hyper_list:\n",
        "    test += 1\n",
        "    # Reset seeds for reproducibility\n",
        "    env.seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    rewards, mean_rewards, eval_returns, visited_states = run_single_test(hyperparams, env)\n",
        "\n",
        "    save_python_data_to_fs(rewards, hyperparams['env_name'], test_name, hyperparams['model_name'], 'rewards')\n",
        "    save_python_data_to_fs(mean_rewards, hyperparams['env_name'], test_name, hyperparams['model_name'], 'mean_rewards')\n",
        "    print('##################### Test ' + str(test) + ' done')\n",
        "  return global_rewards, global_mean_rewards, global_states\n",
        "\n",
        "def save_plot_multiple_test_results_from_file(file_path, env_name, test_name, model_name, plot_name):\n",
        "  data = []\n",
        "  with open(file_path, 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "  plt.figure(figsize=(20,10))\n",
        "  test_i = 0\n",
        "  for mean_rewards in mean_rewards_list:\n",
        "    test_i += 1\n",
        "    plt.plot([i for i in range(len(mean_rewards_list))], mean_rewards, label='Test ' + str(test_i))\n",
        "  plt.legend()\n",
        "\n",
        "  plot_dir = os.path.join('plots', env_name, test_name)\n",
        "  if not os.path.exists(plot_dir):\n",
        "    os.makedirs(plot_dir)\n",
        "  plot_path = plot_dir + '/' + model_name + '_' + plot_name\n",
        "  plt.savefig(plot_path)\n",
        "  print(\"Plots saved in: \" + plot_path)\n",
        "\n",
        "def plot_multiple_test_results_from_file(file_path, labels):\n",
        "  data = []\n",
        "  with open(file_path, 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "  plt.figure(figsize=(20,10))\n",
        "  test_i = 0\n",
        "  for datum in data:\n",
        "    test_i += 1\n",
        "    plt.plot([i for i in range(len(datum))], datum, label='Test ' + str(test_i))\n",
        "  if 'x' in labels:\n",
        "    plt.xlabel(labels['x'], fontsize=FONTSIZE)\n",
        "  if 'y' in labels:\n",
        "    plt.ylabel(labels['y'], fontsize=FONTSIZE)\n",
        "  plt.yticks(fontsize=FONTSIZE)\n",
        "  plt.xticks(fontsize=FONTSIZE)\n",
        "  plt.legend(prop={'size':FONTSIZE})\n",
        "\n",
        "def plot_multiple_files(file_paths, labels, legend_labels):\n",
        "  data = []\n",
        "  for file_path in file_paths:\n",
        "    with open(file_path, 'rb') as fp:\n",
        "      data.append(pickle.load(fp))\n",
        "  plt.figure(figsize=(20,10))\n",
        "  test_i = 0\n",
        "  for i in range(len(data)):\n",
        "    datum = data[i]\n",
        "    test_i += 1\n",
        "    plt.plot([i for i in range(len(datum))], datum, label=legend_labels[i])\n",
        "  if 'x' in labels:\n",
        "    plt.xlabel(labels['x'], fontsize=FONTSIZE)\n",
        "  if 'y' in labels:\n",
        "    plt.ylabel(labels['y'], fontsize=FONTSIZE)\n",
        "  plt.yticks(fontsize=FONTSIZE)\n",
        "  plt.xticks(fontsize=FONTSIZE)\n",
        "  plt.legend(prop={'size':FONTSIZE})\n",
        "\n",
        "def save_python_data_to_fs(data, env_name, test_name, model_name, data_name):\n",
        "  save_dir = os.path.join('saved_data', env_name, test_name, model_name)\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  file_path = os.path.join(save_dir, data_name)\n",
        "  with open(file_path, 'wb') as fp:\n",
        "    pickle.dump(data, fp)\n",
        "  print('Data saved in: ' + file_path)\n",
        "\n",
        "def save_model_to_fs(model, env_name, model_name):\n",
        "  save_dir = os.path.join('keras_models', env_name)\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "  print('Model saved in: ' + model_path)\n",
        "\n",
        "def print_minutes(start_ms, finish_ms):\n",
        "  total_ms = finish_ms - start_ms\n",
        "  seconds = (total_ms / 1000) % 60\n",
        "  minutes = (total_ms / (1000 * 60)) % 60\n",
        "  hours = (total_ms / (1000 * 60 * 60)) % 24\n",
        "  return \"%d:%d:%d\" % (hours, minutes, seconds)\n",
        "\n",
        "def current_ms():\n",
        "  return round(time.time() * 1000)\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "def create_video_from_saved_model(model_path, filename, env_name, num_episodes=5, fps=20):\n",
        "  saved_model = tf_load_model(policy_path)\n",
        "  return create_policy_eval_video(saved_model, filename, env_name, num_episodes, fps)\n",
        "\n",
        "def create_model_eval_video(model, filename, env_name, num_episodes=5, fps=20):\n",
        "  env = gym.make(env_name)\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "\n",
        "  filename = 'videos/' + filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for episode in range(num_episodes):\n",
        "      step = 0\n",
        "      episode_reward = 0\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, input_dim])\n",
        "      video.append_data(env.render(mode='rgb_array'))\n",
        "      episode_done = False\n",
        "      while not episode_done:\n",
        "        step += 1\n",
        "        exploit_actions = model.predict(state)\n",
        "        action = np.argmax(exploit_actions[0])\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        next_state = np.reshape(next_state, [1, input_dim])\n",
        "        state = next_state\n",
        "        episode_done = done\n",
        "        frame = Image.fromarray(env.render(mode='rgb_array'))\n",
        "        if episode_done:\n",
        "          # Close the window\n",
        "          env.close()\n",
        "        frame_draw = ImageDraw.Draw(frame)\n",
        "        frame_draw.text((10, 10), \"Episode: \" + str(episode) , fill =(255, 0, 0))\n",
        "        frame_draw.text((10, 30), \"Step: \" + str(step) , fill =(255, 0, 0))\n",
        "        frame_draw.text((10, 50), \"Reward: \" + str(episode_reward) , fill =(255, 0, 0))\n",
        "        video.append_data(np.asarray(frame))\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "def run_variance_convergence_tests(tests_to_run, hyperparams, env, seed):\n",
        "  env_name = hyperparams['env_name']\n",
        "  test_name = 'same_parameters_variance'\n",
        "\n",
        "  if tests_to_run['dqn']:\n",
        "    print (\"### Start DQN\")\n",
        "    # Single Q network (standard DQN)\n",
        "    s_hyperparams = hyperparams.copy()\n",
        "    s_hyperparams['model_name'] = 'dqn_p1'\n",
        "    s_hyperparams['loss'] = 'MSE'\n",
        "    s_hyperparams['use_target_network'] = False\n",
        "    s_hyperparams['double_q_learning'] = False\n",
        "    s_rewards, s_mean_rewards, s_states = run_multiple_tests(s_hyperparams, env, seed)\n",
        "    save_python_data_to_fs(s_rewards, env_name, test_name, s_hyperparams['model_name'], 's_rewards')\n",
        "    save_python_data_to_fs(s_mean_rewards, env_name, test_name, s_hyperparams['model_name'], 's_mean_rewards')\n",
        "\n",
        "  if tests_to_run['target_dqn_hl']:\n",
        "    print (\"### Start DQN with Target Network and Huber loss\")\n",
        "    # Huber loss\n",
        "    hl_hyperparams = hyperparams.copy()\n",
        "    hl_hyperparams['model_name'] = 'nature_dqn_p1_hl'\n",
        "    hl_hyperparams['use_target_network'] = True\n",
        "    hl_hyperparams['loss'] = 'Huber'\n",
        "    hl_rewards, hl_mean_rewards, hl_states = run_multiple_tests(hl_hyperparams, env, seed)\n",
        "    save_python_data_to_fs(hl_rewards, env_name, test_name, hl_hyperparams['model_name'], 'hl_rewards')\n",
        "    save_python_data_to_fs(hl_mean_rewards, env_name, test_name, hl_hyperparams['model_name'], 'hl_mean_rewards')\n",
        "\n",
        "  if tests_to_run['target_dqn_mse_noclip']:\n",
        "    print (\"### Start DQN with Target Network and MSE\")\n",
        "    # Mean squared error without error clipping\n",
        "    mse_noclip_hyperparams = hyperparams.copy()\n",
        "    mse_noclip_hyperparams['model_name'] = 'nature_dqn_p1_mse_noclip'\n",
        "    mse_noclip_hyperparams['loss'] = 'MSE'\n",
        "    mse_noclip_hyperparams['use_target_network'] = True\n",
        "    mse_noclip_hyperparams['error_clipping'] = False\n",
        "    mse_noclip_rewards, mse_noclip_mean_rewards, mse_noclip_states = run_multiple_tests(mse_noclip_hyperparams, env, seed)\n",
        "    save_python_data_to_fs(mse_noclip_rewards, env_name, test_name, mse_noclip_hyperparams['model_name'], 'mse_noclip_rewards')\n",
        "    save_python_data_to_fs(mse_noclip_mean_rewards, env_name, test_name, mse_noclip_hyperparams['model_name'], 'mse_noclip_mean_rewards')\n",
        "\n",
        "  if tests_to_run['ddqn']:\n",
        "    print (\"### Start DDQN with Huber loss\")\n",
        "    # Huber loss\n",
        "    hl_ddqn_hyperparams = hyperparams.copy()\n",
        "    hl_ddqn_hyperparams['model_name'] = 'ddqn_p1_hl'\n",
        "    hl_ddqn_hyperparams['loss'] = 'Huber'\n",
        "    hl_ddqn_hyperparams['use_target_network'] = True\n",
        "    hl_ddqn_hyperparams['use_double_q_learning'] = True\n",
        "\n",
        "    hl_ddqn_rewards, hl_ddqn_mean_rewards, hl_ddqn_states = run_multiple_tests(hl_ddqn_hyperparams, env, seed)\n",
        "    save_python_data_to_fs(hl_ddqn_rewards, env_name, test_name, hl_ddqn_hyperparams['model_name'], 'hl_ddqn_rewards')\n",
        "    save_python_data_to_fs(hl_ddqn_mean_rewards, env_name, test_name, hl_ddqn_hyperparams['model_name'], 'hl_ddqn_mean_rewards')\n",
        "\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxGD14_4E7Si"
      },
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLAIIYx4E6PO",
        "outputId": "5d77db37-e06f-4b5e-c149-3bb8eee86fa2"
      },
      "source": [
        "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
        "\n",
        "cart_env_name = 'CartPole-v1' \n",
        "\n",
        "cart_env = gym.make(cart_env_name)\n",
        "cart_train_py_env = gym.make(cart_env_name)\n",
        "cart_test_py_env = gym.make(cart_env_name)\n",
        "cart_train_py_env.seed(SEED)\n",
        "cart_train_py_env.seed(SEED * 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deRVD9SFT2J"
      },
      "source": [
        "cart_hyper = {\n",
        "    'env_name': cart_env_name,\n",
        "    'model_name': '',\n",
        "    'layer_params': (128,),\n",
        "    'learning_rate': 0.001,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'use_target_network': True,\n",
        "    'double_q_learning': False,\n",
        "    'target_update_steps': 1000,\n",
        "    'early_stopping': False,\n",
        "    # Do not train if rewards for latest `early_stopping_batch` steps are higher than `early_stopping_limit` on average\n",
        "    'early_stopping_batch': -10,\n",
        "    'early_stopping_limit': 190,\n",
        "    # Stop training altogether if rewards for latest `rewards_mean_episode_limit` are higher than `episode_solved_score` on average\n",
        "    'rewards_mean_episode_limit': -100,\n",
        "    'episode_solved_score': 200,\n",
        "    'max_episode_steps': 200,\n",
        "    'train_episodes': 1000,\n",
        "    'update_frequency': 4,\n",
        "    #'max_train_iterations': 500000, # overrides train_episodes\n",
        "    'batch_size': 64,\n",
        "    'max_buffer_size': 250000,\n",
        "    'explore_only_episodes': -1, #50,\n",
        "    'reward_clipping': False,\n",
        "    'error_clipping': False,\n",
        "    'loss': 'MSE',\n",
        "    'optimizer': 'Adam',\n",
        "    'eval_interval': -1,\n",
        "    'log_episodes': True,\n",
        "    'save_model_every_n_episodes': 400,\n",
        "    'always_save_optimal_models': True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgdIePN9Ob9A"
      },
      "source": [
        "### Training performance variance / convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTRLPtdPOgiW"
      },
      "source": [
        "tests_to_run = {\n",
        "    'dqn': True,\n",
        "    'target_dqn_hl': True,\n",
        "    'target_dqn_mse_noclip': True,\n",
        "    'ddqn': True\n",
        "}\n",
        "run_variance_convergence_tests(tests_to_run, cart_hyper, cart_train_py_env, SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hk7JXNEosJ9"
      },
      "source": [
        "### Model selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAwNICDq91k9"
      },
      "source": [
        "ms_test_name = 'model_selection'\n",
        "varying_vals = {\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "    'epsilon_decay': [0.993, 0.995, 0.997],\n",
        "    'layer_params': [(32,), (32, 32), (64,), (64, 64), (128,), (128, 128)]\n",
        "}\n",
        "hyper_list = generate_hyperparams_for_tests(cart_hyper, varying_vals)\n",
        "run_different_hyperparams(hyper_list, cart_train_py_env, SEED, ms_test_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTUOUCjMVGN_"
      },
      "source": [
        "## MountainCar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J667Ohlch-Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42364f0-287f-43de-bf31-9c7bce9e489f"
      },
      "source": [
        "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
        "\n",
        "mount_env_name = 'MountainCar-v0' \n",
        "\n",
        "mount_env = gym.make(mount_env_name)\n",
        "mount_train_py_env = gym.make(mount_env_name)\n",
        "mount_test_py_env = gym.make(mount_env_name)\n",
        "mount_train_py_env.seed(SEED)\n",
        "mount_train_py_env.seed(SEED * 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg0Q6pZ8izgY"
      },
      "source": [
        "mount_hyper = {\n",
        "    'env_name': mount_env_name,\n",
        "    'model_name': 'dqn_target_p1_2000ts',\n",
        "    'layer_params': (64,),\n",
        "    'learning_rate': 0.001,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.1,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'use_target_network': True,\n",
        "    'double_q_learning': False,\n",
        "    'target_update_steps': 1000,\n",
        "    'early_stopping': True,\n",
        "    # Do not train if rewards for latest `early_stopping_batch` steps are higher than `early_stopping_limit` on average\n",
        "    'early_stopping_batch': -10,\n",
        "    'early_stopping_limit': -100,\n",
        "    # Stop training altogether if rewards for latest `rewards_mean_episode_limit` are higher than `episode_solved_score` on average\n",
        "    'rewards_mean_episode_limit': -100,\n",
        "    'episode_solved_score': -110,\n",
        "    'max_episode_steps': 200,\n",
        "    'train_episodes': 2000,\n",
        "    'update_frequency': 4,\n",
        "    #'max_train_iterations': 500000, # overrides train_episodes\n",
        "    'batch_size': 64,\n",
        "    'max_buffer_size': 250000,\n",
        "    'explore_only_episodes': 100, #50,\n",
        "    'reward_clipping': False,\n",
        "    'error_clipping': True,\n",
        "    'loss': 'Huber',\n",
        "    'optimizer': 'Adam',\n",
        "    'eval_interval': -1,\n",
        "    'log_episodes': True,\n",
        "    'save_model_every_n_episodes': 400,\n",
        "    'always_save_optimal_models': False\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgX09N1iQmtb"
      },
      "source": [
        "### Training performance variance / convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7B180iRQpQg"
      },
      "source": [
        "tests_to_run = {\n",
        "    'dqn': True,\n",
        "    'target_dqn_hl': True,\n",
        "    'target_dqn_mse_noclip': True,\n",
        "    'ddqn': False\n",
        "}\n",
        "run_variance_convergence_tests(tests_to_run, mount_hyper, mount_train_py_env, SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yrPRMHrVK5E"
      },
      "source": [
        "## LunarLander"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jso55Q16Gxwn",
        "outputId": "13503d19-2610-4019-d462-8a9d9ac8d2da"
      },
      "source": [
        "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
        "\n",
        "land_env_name = 'LunarLander-v2'\n",
        "\n",
        "land_train_py_env = gym.make(land_env_name)\n",
        "land_test_py_env = gym.make(land_env_name)\n",
        "land_train_py_env.seed(SEED)\n",
        "land_test_py_env.seed(SEED * 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JjZ-1XSG17P"
      },
      "source": [
        "land_hyper = {\n",
        "    'env_name': land_env_name,\n",
        "    'model_name': '',\n",
        "    'layer_params': (64,64),\n",
        "    'learning_rate': 0.001,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'double_q_learning': True,\n",
        "    'use_target_network': True,\n",
        "    'target_update_steps': 1000,\n",
        "    'early_stopping': True,\n",
        "    # Do not train if rewards for latest `early_stopping_batch` steps are higher than `early_stopping_limit` on average\n",
        "    'early_stopping_batch': -10,\n",
        "    'early_stopping_limit': 180,\n",
        "    # Stop training altogether if rewards for latest `rewards_mean_episode_limit` are higher than `episode_solved_score` on average\n",
        "    'rewards_mean_episode_limit': -100,\n",
        "    'episode_solved_score': 200,\n",
        "    'max_episode_steps': 1000,\n",
        "    'train_episodes': 700,\n",
        "    'update_frequency': 4,\n",
        "    #'max_train_iterations': 500000, # overrides train_episodes\n",
        "    'batch_size': 64,\n",
        "    'max_buffer_size': 250000,\n",
        "    'explore_only_episodes': -1, #50,\n",
        "    'reward_clipping': False,\n",
        "    'error_clipping': True,\n",
        "    'loss': 'Huber',\n",
        "    'optimizer': 'Adam',\n",
        "    'eval_interval': -1,\n",
        "    'log_episodes': True,\n",
        "    'save_model_every_n_episodes': -1,\n",
        "    'always_save_optimal_models': False\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFp4BQtQHQR3"
      },
      "source": [
        "### Training performance variance / convergence\n",
        "\n",
        "The next cell tests the performance variance (and convergence) of a model while training with the same hyperparameters for three times in a row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddk0q1AaeCwF"
      },
      "source": [
        "tests_to_run = {\n",
        "    'dqn': True, \n",
        "    'target_dqn_hl': True,\n",
        "    'target_dqn_mse_noclip': True,\n",
        "    'ddqn': True\n",
        "}\n",
        "run_variance_convergence_tests(tests_to_run, land_hyper, land_train_py_env, SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeiXI8MDK2sb"
      },
      "source": [
        "### Model selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os4vxz6gK4Lh"
      },
      "source": [
        "ms_test_name = 'model_selection'\n",
        "varying_vals = {\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "    'epsilon_decay': [0.993, 0.995, 0.997],\n",
        "    'layer_params': [(32,), (32, 32), (64,), (64, 64), (128,), (128, 128)]\n",
        "}\n",
        "hyper_list = generate_hyperparams_for_tests(land_hyper, varying_vals)\n",
        "run_different_hyperparams(hyper_list, land_train_py_env, SEED, ms_test_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58vnWpsyxGXv"
      },
      "source": [
        "# Model based DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB2jy1ypxEEH"
      },
      "source": [
        "def create_dynamics_predictor_net(hyperparams, env):\n",
        "  _, input_dim = get_output_input_dim_from_env(env)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(hyperparams['dynamics_network']['layer_params'][0], input_dim=input_dim + 1, activation=relu, kernel_initializer=glorot_uniform))\n",
        "  for lr in hyperparams['dynamics_network']['layer_params'][1:]:\n",
        "    model.add(Dense(lr, activation=relu, kernel_initializer=glorot_uniform))\n",
        "  model.add(Dense(input_dim, activation = linear))\n",
        "  model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=hyperparams['dynamics_network']['learning_rate']))\n",
        "  return model\n",
        "\n",
        "# input: array of states Sf = [S1, S2, ...]\n",
        "def compute_mean_covariance(obs_set):\n",
        "  obs = np.transpose(np.array(obs_set))\n",
        "  # Produce an array of means for the different features\n",
        "  mean = np.mean(obs, axis=1)\n",
        "  # Produce covariance matrix for the different features\n",
        "  covariance = np.cov(obs)\n",
        "  return mean, covariance\n",
        "\n",
        "def predict_state(model, state, action):\n",
        "  input = np.reshape(np.append(state[0], action), (1, state.shape[1] + 1))\n",
        "  return model.predict(input)\n",
        "\n",
        "def multivariate_gaussian(state, mean, covariance):\n",
        "  np_state = np.array(state)\n",
        "  return multivariate_normal.pdf(np_state, mean=mean, cov=covariance)\n",
        "\n",
        "def get_explore_action(model, state, action_max, mean, covariance):\n",
        "  predictions = []\n",
        "  for action in range(action_max):\n",
        "    state_prediction = predict_state(model, state, action)\n",
        "    predictions.append(multivariate_gaussian(state_prediction, mean, covariance))\n",
        "  return np.argmin(predictions)\n",
        "\n",
        "def learn_dynamics_network(hyperparams, buffer, dynamics_network):\n",
        "  # Skip learning until we have enough data in the buffer\n",
        "  if len(buffer) < hyperparams['dynamics_network']['batch_size']:\n",
        "    return\n",
        "\n",
        "  sample_batch = get_batch_from_buffer(buffer, hyperparams['dynamics_network']['batch_size'])\n",
        "  states, actions, _, next_states, _ = extract_data_from_batch(sample_batch)\n",
        "  states_actions = np.append(states, np.reshape(actions, (hyperparams['dynamics_network']['batch_size'], 1)), axis = 1)\n",
        "  history = dynamics_network.fit(states_actions, next_states, epochs=1, verbose=0)\n",
        "\n",
        "def extract_states_from_buffer_samples(samples):\n",
        "  states = []\n",
        "  for sample in samples:\n",
        "    states.append(np.squeeze(sample[0]))\n",
        "  return states\n",
        "\n",
        "def get_action_model_based(hyperparams, episode, state, epsilon, model, dynamics_network, max_action, buffer):\n",
        "  explore = False\n",
        "  if episode < hyperparams['explore_only_episodes'] or np.random.rand() < epsilon:\n",
        "    explore = True\n",
        "  \n",
        "  # Explore: use dynamics network to select action which will lead to least likely next state\n",
        "  if explore:\n",
        "    start_index = hyperparams['dynamics_network']['F']\n",
        "    # Account for initial states\n",
        "    if len(buffer) < abs(start_index):\n",
        "      start_index = -1 * len(buffer)\n",
        "    Sf = [buffer[i] for i in range(start_index, 0)]\n",
        "    # For the very first steps we must return a purely random action\n",
        "    if len(buffer) < hyperparams['dynamics_network']['batch_size']:\n",
        "      return random.randrange(max_action)\n",
        "    Sf_states = extract_states_from_buffer_samples(Sf)\n",
        "    mean, covariance = compute_mean_covariance(Sf_states)\n",
        "    action = get_explore_action(dynamics_predictor, state, max_action, mean, covariance)\n",
        "    return action\n",
        "\n",
        "  # Exploit: select best predicted action\n",
        "  exploit_actions = model.predict(state)\n",
        "  return np.argmax(exploit_actions[0])\n",
        "\n",
        "def train_model_based(hyperparams, env, eval_env, model, target_model, dynamics_network, buffer):\n",
        "  start_time = current_ms()\n",
        "  # Global iteration counter\n",
        "  iteration = 0\n",
        "  episode = -1\n",
        "  # List of episode rewards\n",
        "  episode_rewards = []\n",
        "  mean_rewards = []\n",
        "  visited_states = []\n",
        "  eval_returns = []\n",
        "  epsilon = hyperparams['epsilon_start']\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "\n",
        "  while not train_done(hyperparams, episode, iteration):\n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "    visited_states.append(state)\n",
        "    state = np.reshape(state, [1, input_dim])\n",
        "    for step in range(hyperparams['max_episode_steps']):\n",
        "      action = get_action_model_based(hyperparams, episode, state, epsilon, model, dynamics_network, output_dim, buffer)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      if hyperparams['reward_clipping']:\n",
        "        if reward < -1:\n",
        "          reward = -1\n",
        "        elif reward > 1:\n",
        "          reward = 1\n",
        "      visited_states.append(next_state)\n",
        "      episode_reward += reward\n",
        "      next_state = np.reshape(next_state, [1, input_dim])\n",
        "      sample = (state, action, reward, next_state, done)\n",
        "      add_sample_to_buffer(hyperparams, buffer, sample)\n",
        "      state = next_state\n",
        "      iteration += 1\n",
        "\n",
        "      if not explore_only(hyperparams, episode, buffer) and iteration % hyperparams['update_frequency'] == 0:\n",
        "        learn(hyperparams, output_dim, episode_rewards, buffer, model, target_model)\n",
        "      \n",
        "      # Every C steps set Q' = Q\n",
        "      if hyperparams['use_target_network'] and not explore_only(hyperparams, episode, buffer) and iteration % hyperparams['target_update_steps']:\n",
        "        target_model.set_weights(model.get_weights()) \n",
        "\n",
        "      # Train dynamics predictor\n",
        "      learn_dynamics_network(hyperparams, buffer, dynamics_network)\n",
        "\n",
        "      if hyperparams['eval_interval'] > 0 and iteration % hyperparams['eval_interval'] == 0:\n",
        "        avg_returns, _ = eval_model(hyperparams, model, eval_env)\n",
        "        eval_returns.append(avg_returns)\n",
        "        print(\">>> Iteration {0}, Episode {1}, Avg. returns {2}\".format(iteration, episode, avg_returns))\n",
        "\n",
        "      # Episode done\n",
        "      if done:\n",
        "        break\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    # Save model to fs\n",
        "    if hyperparams['save_model_every_n_episodes'] > 0 and episode % hyperparams['save_model_every_n_episodes'] == 0:\n",
        "      model_name = hyperparams['model_name'] + '_ep' + str(episode)\n",
        "      if episode_reward > hyperparams['episode_solved_score']:\n",
        "        model_name += '_SOLVED'\n",
        "      save_model_to_fs(model, hyperparams['env_name'], model_name)\n",
        "    elif hyperparams['always_save_optimal_models'] and episode_reward > hyperparams['episode_solved_score']:\n",
        "      model_name = hyperparams['model_name'] + '_ep' + str(episode) + '_SOLVED'\n",
        "      save_model_to_fs(model, hyperparams['env_name'], model_name)\n",
        "\n",
        "    # Epsilon decay\n",
        "    if not explore_only(hyperparams, episode, buffer) and hyperparams['epsilon_decay'] > 0 and epsilon > hyperparams['epsilon_end']:\n",
        "      epsilon *= hyperparams['epsilon_decay']\n",
        "    \n",
        "    # Early stopping\n",
        "    last_rewards_mean = np.mean(episode_rewards[hyperparams['rewards_mean_episode_limit']:])\n",
        "    mean_rewards.append(last_rewards_mean)\n",
        "    if hyperparams['early_stopping'] and last_rewards_mean > hyperparams['episode_solved_score']:\n",
        "        print(\"Training complete with early stopping\")\n",
        "        break\n",
        "\n",
        "    if hyperparams['log_episodes']:\n",
        "      print(\"Iteration {0}, Episode {1}, Reward {2}, Avg. reward {3}, epsilon {4}\".format(iteration, episode, episode_reward, last_rewards_mean, epsilon))\n",
        "\n",
        "  end_time = current_ms()\n",
        "  print(\"Evaluation done in\", print_minutes(start_time, end_time))\n",
        "  return episode_rewards, mean_rewards, eval_returns, visited_states\n",
        "\n",
        "def plot_visited_states(states, shape):\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.scatter([state[shape[0]] for state in states], [state[shape[1]] for state in states])\n",
        "\n",
        "def plot_visited_states_from_file(file_path, shape, limit=(50*200)):\n",
        "  data = []\n",
        "  with open(file_path, 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.scatter([state[shape[0]] for state in data[:limit]], [state[shape[1]] for state in data[:limit]])\n",
        "  plt.xlabel('Posizione', fontsize=FONTSIZE)\n",
        "  plt.ylabel('Velocit', fontsize=FONTSIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY_WDBkqSFD"
      },
      "source": [
        "## MountainCar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEuUjxW_qRns",
        "outputId": "ff98e4b6-4962-4499-9b4f-04045d941433"
      },
      "source": [
        "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
        "\n",
        "mount_env_name = 'MountainCar-v0' \n",
        "\n",
        "mount_env = gym.make(mount_env_name)\n",
        "mount_train_py_env = gym.make(mount_env_name)\n",
        "mount_test_py_env = gym.make(mount_env_name)\n",
        "mount_train_py_env.seed(SEED)\n",
        "mount_train_py_env.seed(SEED * 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UFnBouS0JqU"
      },
      "source": [
        "mount_hyper = {\n",
        "    'env_name': mount_env_name,\n",
        "    'model_name': 'dqn_model_based_p1_2000ts',\n",
        "    'layer_params': (64,),\n",
        "    'learning_rate': 0.001,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'use_target_network': True,\n",
        "    'double_q_learning': False,\n",
        "    'target_update_steps': 1000,\n",
        "    'early_stopping': True,\n",
        "    # Do not train if rewards for latest `early_stopping_batch` steps are higher than `early_stopping_limit` on average\n",
        "    'early_stopping_batch': -10,\n",
        "    'early_stopping_limit': -100,\n",
        "    # Stop training altogether if rewards for latest `rewards_mean_episode_limit` are higher than `episode_solved_score` on average\n",
        "    'rewards_mean_episode_limit': -100,\n",
        "    'episode_solved_score': -110,\n",
        "    'max_episode_steps': 200,\n",
        "    'train_episodes': 2000,\n",
        "    'update_frequency': 4,\n",
        "    #'max_train_iterations': 500000, # overrides train_episodes\n",
        "    'batch_size': 64,\n",
        "    'max_buffer_size': 250000,\n",
        "    'explore_only_episodes': 100, #50,\n",
        "    'reward_clipping': False,\n",
        "    'error_clipping': True,\n",
        "    'loss': 'Huber',\n",
        "    'optimizer': 'Adam',\n",
        "    'eval_interval': -1,\n",
        "    'log_episodes': True,\n",
        "    'save_model_every_n_episodes': 400,\n",
        "    'always_save_optimal_models': False,\n",
        "    'dynamics_network': {\n",
        "        'layer_params': (24, 24),\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 0.02,\n",
        "        'F': -50\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHovqli6hz7I"
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "mount_train_py_env.seed(SEED)\n",
        "\n",
        "test_name = 'same_parameter_variance'\n",
        "\n",
        "for i in range(3):\n",
        "  model = create_dqn_network(mount_hyper, mount_train_py_env)\n",
        "  target_model = None\n",
        "  if mount_hyper['use_target_network']:\n",
        "    target_model = create_dqn_network(mount_hyper, mount_train_py_env)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "  buffer = create_replay_buffer(mount_hyper)\n",
        "  rewards, mean_rewards, eval_returns, visited_states = train(mount_hyper, mount_train_py_env, mount_test_py_env, model, target_model, buffer)\n",
        "  save_python_data_to_fs(mean_rewards, mount_env_name, test_name, mount_hyper['model_name'], 'mean_rewards_' + str(i))\n",
        "  save_model_to_fs(model, mount_env_name, mount_hyper['model_name'] + '_1000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CJg9e5DmRzy"
      },
      "source": [
        "# Prioritized Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhJZwc_WmYbf"
      },
      "source": [
        "# Sum-tree functions\n",
        "# The samples array contains the experiences\n",
        "# The p_tree array contains couples (sum of priorities of sub-tree leaves, max priority in sub-tree leaves)\n",
        "def st_get_tree(hyperparams):\n",
        "  # Array containing actual samples\n",
        "  samples = np.zeros(hyperparams['max_buffer_size'], dtype=object)\n",
        "  # Binary which leaves contain the probability of each sample\n",
        "  p_tree = np.zeros(hyperparams['max_buffer_size'] * 2 - 1, dtype=object)\n",
        "  for i in range(hyperparams['max_buffer_size'] * 2 - 1):\n",
        "    p_tree[i] = (0, 0)\n",
        "  return {'p_tree': p_tree, 'samples': samples, 'current_idx': 0}\n",
        "\n",
        "def st_propagate_change(st, idx, probability_change, max_priority):\n",
        "  # Propagate the probability change up the tree\n",
        "  parent_node = (idx - 1) // 2\n",
        "  max_p = max(st['p_tree'][parent_node][1], max_priority)\n",
        "  st['p_tree'][parent_node] = (st['p_tree'][parent_node][0] + probability_change, max_p)\n",
        "  # Check for termination\n",
        "  if parent_node == 0:\n",
        "    return\n",
        "  else:\n",
        "    return st_propagate_change(st, parent_node, probability_change, max_p)\n",
        "\n",
        "def st_add(st, priority, sample):\n",
        "  # Add sample to array of samples\n",
        "  st['samples'][st['current_idx']] = sample\n",
        "  # Create sum-tree leaf value\n",
        "  leaf_idx = st['current_idx'] + len(st['samples']) - 1\n",
        "\n",
        "  # Calculate the change in priority\n",
        "  priority_change = priority - st['p_tree'][leaf_idx][0]\n",
        "  st['p_tree'][leaf_idx] = (priority, priority)\n",
        "\n",
        "  # Propagate the priority change up the tree\n",
        "  st_propagate_change(st, leaf_idx, priority_change, priority)\n",
        "  \n",
        "  # Update index of written samples\n",
        "  st['current_idx'] += 1\n",
        "  if st['current_idx'] >= len(st['samples']):\n",
        "    st['current_idx'] = 0\n",
        "\n",
        "def st_update(st, idx, error):\n",
        "  priority_change = error - st['p_tree'][idx][0]\n",
        "  max_p = max(error, st['p_tree'][idx][1])\n",
        "  st['p_tree'][idx] = (error, max_p)\n",
        "  st_propagate_change(st, idx, priority_change, max_p)\n",
        "\n",
        "def st_recursive_get(st, idx, p):\n",
        "  # Calc indexes of left and right children\n",
        "  left = 2 * idx + 1\n",
        "  right = left + 1\n",
        "\n",
        "  # Termination condition\n",
        "  if left >= len(st['p_tree']):\n",
        "    return idx\n",
        "\n",
        "  # Keep looking for correct priority index down the tree\n",
        "  if p <= st['p_tree'][left][0]:\n",
        "    return st_recursive_get(st, left, p)\n",
        "  else:\n",
        "    return st_recursive_get(st, right, p - st['p_tree'][left][0])\n",
        "\n",
        "def st_get(st, p):\n",
        "  idx = st_recursive_get(st, 0, p)\n",
        "  sample_idx = idx - len(st['samples']) + 1\n",
        "\n",
        "  return (idx, st['p_tree'][idx][0], st['samples'][sample_idx])\n",
        "\n",
        "def st_get_samples(st, batch_size):\n",
        "  samples = []\n",
        "  # Extract samples randomly from `batch_size` segments stored in the sum-tree\n",
        "  segment_idx = st['p_tree'][0][0] / batch_size\n",
        "  for i in range(batch_size):\n",
        "    segment_start = segment_idx * i\n",
        "    segment_end = segment_idx * (i + 1)\n",
        "\n",
        "    p = random.uniform(segment_start, segment_end)\n",
        "    (idx, p, sample) = st_get(st, p)\n",
        "    samples.append((idx, p, sample))\n",
        "  return samples\n",
        "\n",
        "# Note: this is a problem if we have negative rewards/errors\n",
        "def st_get_priority(hyperparams, error):\n",
        "  return (error + (hyperparams['PER']['e'])) ** hyperparams['PER']['a']\n",
        "\n",
        "def st_add_sample(hyperparams, st, error, sample):\n",
        "  priority = st_get_priority(hyperparams, error)\n",
        "  st_add(st, priority, sample)\n",
        "\n",
        "def st_update_sample(hyperparams, st, error, sample_idx):\n",
        "  priority = st_get_priority(hyperparams, error)\n",
        "  st_update(st, sample_idx, priority)\n",
        "\n",
        "# PER DQN functions\n",
        "\n",
        "def extract_data_from_st_batch(st_batch):\n",
        "  states = np.array([i[2][0] for i in st_batch])\n",
        "  actions = np.array([i[2][1] for i in st_batch])\n",
        "  rewards = np.array([i[2][2] for i in st_batch])\n",
        "  next_states = np.array([i[2][3] for i in st_batch])\n",
        "  dones = np.array([i[2][4] for i in st_batch])\n",
        "  idxs = np.array([i[0] for i in st_batch])\n",
        "  priorities = np.array([i[1] for i in st_batch])\n",
        "\n",
        "  # Squeeze: remove axes of length 1 from array\n",
        "  states = np.squeeze(states, axis=1)\n",
        "  next_states = np.squeeze(next_states, axis=1)\n",
        "  return states, actions, rewards, next_states, dones, idxs, priorities\n",
        "\n",
        "def learn_per(hyperparams, output_dim, rewards, buffer, model, target_model):\n",
        "  # Early stopping: do not train agent if it is performing well\n",
        "  if hyperparams['early_stopping'] and np.mean(rewards[hyperparams['early_stopping_batch']:]) > hyperparams['early_stopping_limit']:\n",
        "    return\n",
        "  \n",
        "  sample_batch = st_get_samples(buffer, hyperparams['batch_size'])\n",
        "  states, actions, rewards, next_states, dones, idxs, priorities = extract_data_from_st_batch(sample_batch)\n",
        "\n",
        "  # Calc Q(s, a)\n",
        "  model_s_predictions = model.predict_on_batch(states)\n",
        "\n",
        "  # Calc Q(s', a')\n",
        "  model_ns_predictions = model.predict_on_batch(next_states)\n",
        "\n",
        "  # Calc Q'(s', a')\n",
        "  target_model_ns_predictions = target_model.predict_on_batch(next_states)\n",
        "\n",
        "  errors = np.zeros(hyperparams['batch_size'])\n",
        "\n",
        "  target_vec = np.zeros((hyperparams['batch_size'], output_dim))\n",
        "\n",
        "  # Calc TD errors\n",
        "  for i in range(hyperparams['batch_size']):\n",
        "    target = np.zeros(output_dim)\n",
        "    if dones[i]:\n",
        "      target[actions[i]] = rewards[i]\n",
        "    else:\n",
        "      target[actions[i]] = rewards[i] + hyperparams['gamma'] * target_model_ns_predictions[i][np.argmax(model_ns_predictions[i])]\n",
        "\n",
        "    errors[i] = abs(model_s_predictions[i][actions[i]] - target[actions[i]])\n",
        "    target_vec[i] = target\n",
        "\n",
        "    # Update sample weights\n",
        "    st_update_sample(hyperparams, buffer, errors[i], idxs[i]) \n",
        "\n",
        "  model.fit(states, target_vec, epochs=1, batch_size=hyperparams['batch_size'], verbose=0)\n",
        "\n",
        "def train_per(hyperparams, env, eval_env, model, target_model, buffer):\n",
        "  start_time = current_ms()\n",
        "  # Global iteration counter\n",
        "  iteration = 0\n",
        "  episode = -1\n",
        "  # List of episode rewards\n",
        "  episode_rewards = []\n",
        "  mean_rewards = []\n",
        "  visited_states = []\n",
        "  eval_returns = []\n",
        "  epsilon = hyperparams['epsilon_start']\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "\n",
        "  while not train_done(hyperparams, episode, iteration):\n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "    visited_states.append(state)\n",
        "    state = np.reshape(state, [1, input_dim])\n",
        "    for step in range(hyperparams['max_episode_steps']):\n",
        "      action = get_action(hyperparams, episode, state, epsilon, model, output_dim)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      if hyperparams['reward_clipping']:\n",
        "        if reward < -1:\n",
        "          reward = -1\n",
        "        elif reward > 1:\n",
        "          reward = 1\n",
        "      visited_states.append(next_state)\n",
        "      episode_reward += reward\n",
        "      next_state = np.reshape(next_state, [1, input_dim])\n",
        "      sample = (state, action, reward, next_state, done)\n",
        "      # Add sample with max priority (stored in the first element of the buffer sum-tree)\n",
        "      st_add_sample(hyperparams, buffer, buffer['p_tree'][0][1], sample)\n",
        "      state = next_state\n",
        "      iteration += 1\n",
        "\n",
        "      #if not explore_only(hyperparams, episode, buffer) and iteration % hyperparams['update_frequency'] == 0:\n",
        "      if episode > hyperparams['start_training_after'] and iteration % hyperparams['update_frequency'] == 0:\n",
        "        learn_per(hyperparams, output_dim, episode_rewards, buffer, model, target_model)\n",
        "      \n",
        "      # Every C steps set Q' = Q\n",
        "      if hyperparams['use_target_network'] and not explore_only(hyperparams, episode, buffer) and iteration % hyperparams['target_update_steps']:\n",
        "        target_model.set_weights(model.get_weights()) \n",
        "\n",
        "      if hyperparams['eval_interval'] > 0 and iteration % hyperparams['eval_interval'] == 0:\n",
        "        avg_returns, _ = eval_model(hyperparams, model, eval_env)\n",
        "        eval_returns.append(avg_returns)\n",
        "        print(\">>> Iteration {0}, Episode {1}, Avg. returns {2}\".format(iteration, episode, avg_returns))\n",
        "\n",
        "      # Episode done\n",
        "      if done:\n",
        "        break\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    # Save model to fs\n",
        "    if hyperparams['save_model_every_n_episodes'] > 0 and episode % hyperparams['save_model_every_n_episodes'] == 0:\n",
        "      model_name = hyperparams['model_name'] + '_ep' + str(episode)\n",
        "      if episode_reward > hyperparams['episode_solved_score']:\n",
        "        model_name += '_SOLVED'\n",
        "      save_model_to_fs(model, hyperparams['env_name'], model_name)\n",
        "    elif hyperparams['always_save_optimal_models'] and episode_reward > hyperparams['episode_solved_score']:\n",
        "      model_name = hyperparams['model_name'] + '_ep' + str(episode) + '_SOLVED'\n",
        "      save_model_to_fs(model, hyperparams['env_name'], model_name)\n",
        "\n",
        "    # Epsilon decay\n",
        "    if not explore_only(hyperparams, episode, buffer) and hyperparams['epsilon_decay'] > 0 and epsilon > hyperparams['epsilon_end']:\n",
        "      epsilon *= hyperparams['epsilon_decay']\n",
        "    \n",
        "    # Early stopping\n",
        "    last_rewards_mean = np.mean(episode_rewards[hyperparams['rewards_mean_episode_limit']:])\n",
        "    mean_rewards.append(last_rewards_mean)\n",
        "    if hyperparams['early_stopping'] and last_rewards_mean > hyperparams['episode_solved_score']:\n",
        "        print(\"Training complete with early stopping\")\n",
        "        break\n",
        "\n",
        "    if hyperparams['log_episodes']:\n",
        "      print(\"Iteration {0}, Episode {1}, Reward {2}, Avg. reward {3}, epsilon {4}\".format(iteration, episode, episode_reward, last_rewards_mean, epsilon))\n",
        "      #print(\">>>>\", buffer['p_tree'][0])\n",
        "  end_time = current_ms()\n",
        "  print(\"Evaluation done in\", print_minutes(start_time, end_time))\n",
        "  return episode_rewards, mean_rewards, eval_returns, visited_states\n",
        "\n",
        "def fill_memory(hyperparams, buffer, env):\n",
        "  output_dim, input_dim = get_output_input_dim_from_env(env)\n",
        "  state = env.reset()\n",
        "  state = np.reshape(state, [1, input_dim])\n",
        "  while True:\n",
        "    action = random.randrange(output_dim)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    next_state = np.reshape(next_state, [1, input_dim])\n",
        "    sample = (state, action, reward, next_state, done)\n",
        "    st_add_sample(hyperparams, buffer, max(reward, 0.0), sample)\n",
        "    state = next_state\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, input_dim])\n",
        "    if buffer['current_idx'] == 0:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJw67E9fEftm"
      },
      "source": [
        "## MountainCar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AemOoRSOElci",
        "outputId": "14c95481-e028-451e-8722-b00fbd427423"
      },
      "source": [
        "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
        "\n",
        "mount_env_name = 'MountainCar-v0' \n",
        "\n",
        "mount_env = gym.make(mount_env_name)\n",
        "mount_train_py_env = gym.make(mount_env_name)\n",
        "mount_test_py_env = gym.make(mount_env_name)\n",
        "mount_train_py_env.seed(SEED)\n",
        "mount_train_py_env.seed(SEED * 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mksVU7_PEsPt"
      },
      "source": [
        "mount_hyper = {\n",
        "    'env_name': mount_env_name,\n",
        "    'model_name': 'dqn_PER2',\n",
        "    'layer_params': (64,),\n",
        "    'learning_rate': 0.001,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.1,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'double_q_learning': False,\n",
        "    'use_target_network': True,\n",
        "    'target_update_steps': 1000,\n",
        "    'early_stopping': True,\n",
        "    # Do not train if rewards for latest `early_stopping_batch` steps are higher than `early_stopping_limit` on average\n",
        "    'early_stopping_batch': -10,\n",
        "    'early_stopping_limit': -100,\n",
        "    # Stop training altogether if rewards for latest `rewards_mean_episode_limit` are higher than `episode_solved_score` on average\n",
        "    'rewards_mean_episode_limit': -100,\n",
        "    'episode_solved_score': -110,\n",
        "    'max_episode_steps': 200,\n",
        "    'train_episodes': 2000,\n",
        "    'update_frequency': 4,\n",
        "    #'max_train_iterations': 500000, # overrides train_episodes\n",
        "    'batch_size': 64,\n",
        "    'max_buffer_size': 250000,\n",
        "    'start_training_after': 20,\n",
        "    'explore_only_episodes': 100,\n",
        "    'reward_clipping': False,\n",
        "    'error_clipping': True,\n",
        "    'loss': 'Huber',\n",
        "    'optimizer': 'Adam',\n",
        "    'eval_interval': -1,\n",
        "    'log_episodes': True,\n",
        "    'save_model_every_n_episodes': -1,\n",
        "    'always_save_optimal_models': False,\n",
        "    'minimum_reward': -1,\n",
        "    'maximum_reward': 1,\n",
        "    'dynamics_network': {\n",
        "        'layer_params': (24, 24),\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 0.02,\n",
        "        'F': 50\n",
        "    },\n",
        "    'PER': {\n",
        "        'e': 0.01,\n",
        "        'a': 0.6,\n",
        "        'b': 0.5\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrWCkA1dCaRE"
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "mount_train_py_env.seed(SEED)\n",
        "\n",
        "test_name = 'same_parameter_variance_per'\n",
        "\n",
        "for i in range(3):\n",
        "  model = create_dqn_network(mount_hyper, mount_train_py_env)\n",
        "  target_model = None\n",
        "  if mount_hyper['use_target_network']:\n",
        "    target_model = create_dqn_network(mount_hyper, mount_train_py_env)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "  buffer = st_get_tree(mount_hyper)\n",
        "  fill_memory(mount_hyper, buffer, mount_train_py_env)\n",
        "  rewards, mean_rewards, eval_returns, visited_states = train_per(mount_hyper, mount_train_py_env, mount_test_py_env, model, target_model, buffer)\n",
        "  save_python_data_to_fs(mean_rewards, mount_env_name, test_name, mount_hyper['model_name'], 'mean_rewards_' + str(i))\n",
        "  save_model_to_fs(model, mount_env_name, mount_hyper['model_name'] + '_2000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbS-LEu8JYdN"
      },
      "source": [
        "## LunarLander"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmmJ2n0oJdN4",
        "outputId": "54f136ff-0734-4f3b-fe2a-d13794f89f30"
      },
      "source": [
        "# https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
        "\n",
        "land_env_name = 'LunarLander-v2'\n",
        "\n",
        "land_train_py_env = gym.make(land_env_name)\n",
        "land_test_py_env = gym.make(land_env_name)\n",
        "land_train_py_env.seed(SEED)\n",
        "land_test_py_env.seed(SEED * 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_OLmKnJJifh"
      },
      "source": [
        "land_hyper = {\n",
        "    'env_name': land_env_name,\n",
        "    'model_name': 'dqn_PER2',\n",
        "    'layer_params': (64,64),\n",
        "    'learning_rate': 0.001,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'double_q_learning': False,\n",
        "    'use_target_network': True,\n",
        "    'target_update_steps': 1000,\n",
        "    'early_stopping': True,\n",
        "    # Do not train if rewards for latest `early_stopping_batch` steps are higher than `early_stopping_limit` on average\n",
        "    'early_stopping_batch': -10,\n",
        "    'early_stopping_limit': 180,\n",
        "    # Stop training altogether if rewards for latest `rewards_mean_episode_limit` are higher than `episode_solved_score` on average\n",
        "    'rewards_mean_episode_limit': -100,\n",
        "    'episode_solved_score': 200,\n",
        "    'max_episode_steps': 1000,\n",
        "    'train_episodes': 700,\n",
        "    'update_frequency': 4,\n",
        "    #'max_train_iterations': 500000, # overrides train_episodes\n",
        "    'batch_size': 64,\n",
        "    'max_buffer_size': 250000,\n",
        "    'explore_only_episodes': 100, #50,\n",
        "    'start_training_after': 0,\n",
        "    'reward_clipping': False,\n",
        "    'error_clipping': True,\n",
        "    'loss': 'Huber',\n",
        "    'optimizer': 'Adam',\n",
        "    'eval_interval': -1,\n",
        "    'log_episodes': True,\n",
        "    'save_model_every_n_episodes': -1,\n",
        "    'always_save_optimal_models': False,\n",
        "    'PER': {\n",
        "        'e': 0.01,\n",
        "        'a': 0.6,\n",
        "        'b': 0.5\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNTvbh34Ju2v"
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "mount_train_py_env.seed(SEED)\n",
        "\n",
        "test_name = 'same_parameter_variance_per'\n",
        "\n",
        "for i in range(3):\n",
        "  model = create_dqn_network(land_hyper, land_train_py_env)\n",
        "  target_model = None\n",
        "  if land_hyper['use_target_network']:\n",
        "    target_model = create_dqn_network(land_hyper, land_train_py_env)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "  buffer = st_get_tree(land_hyper)\n",
        "  fill_memory(land_hyper, buffer, land_train_py_env)\n",
        "  rewards, mean_rewards, eval_returns, visited_states = train_per(land_hyper, land_train_py_env, land_test_py_env, model, target_model, buffer)\n",
        "  save_python_data_to_fs(mean_rewards, land_env_name, test_name, land_hyper['model_name'], 'mean_rewards_' + str(i))\n",
        "  save_model_to_fs(model, land_env_name, land_hyper['model_name'] + '_2000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBDDKp6rnkas"
      },
      "source": [
        "# Helpful references and sources of inspiration\n",
        "# https://github.com/fakemonk1/Reinforcement-Learning-Lunar_Lander\n",
        "# https://danieltakeshi.github.io/2019/07/14/per/\n",
        "# https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
        "# https://github.com/jaromiru/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
        "# https://wingedsheep.com/lunar-lander-dqn/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}